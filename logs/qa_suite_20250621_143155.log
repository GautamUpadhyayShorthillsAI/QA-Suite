2025-06-21 14:31:55 | INFO     | Backend | info:76 | Starting QA-Suite Backend | Context: {"python_version": "3.12.3 (main, May 26 2025, 18:50:19) [GCC 13.3.0]", "flask_version": "2.3.3", "langchain_version": "0.1.53"}
2025-06-21 14:31:55 | INFO     | Backend | info:76 | LLM initialized | Context: {"model": "gemini-2.5-flash-preview-05-20", "temperature": 0.2, "api_key_configured": true}
2025-06-21 14:31:55 | INFO     | Backend | info:76 | Starting Flask development server | Context: {"host": "localhost", "port": 5000, "debug": true}
2025-06-21 15:01:06 | INFO     | Backend | info:76 | Received test ideas generation request
2025-06-21 15:01:06 | INFO     | Backend | info:76 | Processing test ideas request | Context: {"functionality": "make test cases for login and first form", "js_content_length": 3054, "has_js_content": true}
2025-06-21 15:01:06 | INFO     | Backend | info:76 | Calling LLM for test ideas generation
2025-06-21 15:01:18 | INFO     | Backend | info:76 | LLM response received | Context: {"response_length": 1846, "llm_duration": 12.683499336242676}
2025-06-21 15:01:18 | INFO     | Backend | info:76 | Test ideas generated successfully | Context: {"test_ideas_count": 20, "ideas_preview": ["Verify login page loads successfully.", "Test successful login with valid username and password.", "Attempt login with empty username field and verify no navigation occurs."]}
2025-06-21 15:01:18 | INFO     | QA-Suite | performance:157 | PERFORMANCE generate_test_ideas | Duration: 12.68s | Context: {"test_ideas_count": 20}
2025-06-21 15:01:21 | INFO     | Backend | info:76 | Received script generation request
2025-06-21 15:01:21 | INFO     | Backend | info:76 | Processing script generation request | Context: {"website_url": "123", "selected_tests_count": 20, "js_content_length": 3054, "selected_tests_preview": ["Verify login page loads successfully.", "Test successful login with valid username and password.", "Attempt login with empty username field and verify no navigation occurs."]}
2025-06-21 15:01:21 | INFO     | Backend | info:76 | Calling LLM for script generation
